# **Sentiment Analysis Using DistilBERT & Machine Learning Models**

ğŸš€ A deep learning-based sentiment analysis project using DistilBERT embeddings and multiple classifiers, including Logistic Regression, Random Forest, and Gradient Boosting.

ğŸ“Œ **Project Overview**

This project performs sentiment analysis on the IMDB dataset, classifying reviews as either positive or negative. We use DistilBERT, a lighter version of BERT, to extract text embeddings, which are then used for training deep learning and machine learning models.

ğŸ“‚ **Dataset**

Dataset Name: IMDB Dataset

Number of Classes: 2 (Positive, Negative)

Number of Records: 50,000 (Balanced)


ğŸ› ï¸ **Technologies & Libraries:**

Python

TensorFlow/Keras

Hugging Face Transformers (DistilBERT)

Scikit-learn

Pandas

NumPy

NLTK

Joblib


ğŸ”¨ **Project Workflow**

1ï¸âƒ£ Load Dataset

2ï¸âƒ£ Preprocess Text (lowercase, remove special characters, stopwords, stemming)

3ï¸âƒ£ Extract DistilBERT embeddings

4ï¸âƒ£ Train Multiple Models (Deep Learning, Logistic Regression, Random Forest, Gradient Boosting)

5ï¸âƒ£ Evaluate Performance

6ï¸âƒ£ Save Trained Models

7ï¸âƒ£ Make Predictions on New Reviews

ğŸ§©**Model Architectures**

Deep Learning Model

Input Layer: 768 (DistilBERT embedding size)
Hidden Layer: 128 neurons, ReLU activation
Dropout: 0.3
Output Layer: 1 neuron, Sigmoid activation

Machine Learning Models
Logistic Regression
Random Forest (200 estimators)
Gradient Boosting (100 estimators, learning rate 0.1)

ğŸ¯**Performance Metrics**

Model	Train Accuracy	            Test   Accuracy

Deep Learning (DistilBERT)	      ~50%	  ~78%

Logistic Regression	              ~79%	  ~78%

Gradient Boosting	                ~76%	  ~74%

Random Forest	                    ~100% 	~73%

ğŸ“Œ Note: The Deep Learning model underfits, while Random Forest overfits.

ğŸ“Œ **Key Takeaways**

âœ… Used DistilBERT embeddings for better text understanding

âœ… Compared performance of Deep Learning & ML models

âœ… Identified issues like underfitting & overfitting

âœ… Saved trained models for future predictions

